for eg. if I take a sentence "Cat is Cat" , then cat is used multiple times here but token embedding will provide the same vector 
embedding but they are used at the different positions in the sentence. So , to overlook on this problem we use Positional embedding.


<img width="761" height="257" alt="image" src="https://github.com/user-attachments/assets/4a6934e4-5ee9-4549-a443-4c2cc5a0e7c3" />

here positonal embeddings are taken into consideration along with token embedding to produce final input embedding .(Absolute}
another type of embedding that is used large models is (relative embedding).


